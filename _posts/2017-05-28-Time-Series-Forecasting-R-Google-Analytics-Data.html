---
title: "Time Series Forecasting in R with Google Analytics Data"
author: "Felipe Rego"
date: "May, 2017"
output:
  html_document:
    keep_md: yes
---

Data that are obtained in series of points over an equally spaced period of time are generally referred to as Time series data. Monthly retail sales, daily weather forecast, unemployment figures, consumer sentiment surveys, among many others, are classic examples of time series data. In fact, most variables in nature, science, business and many other applications rely on data that can be measured in a fixed time interval.

One of the key reasons time series data are analysed is to understand the past and predict the future. Scientists can use historical data on climate to predict future climate changes. Marketing managers can look at historical sales of a certain product and predict demand in the future.

In the digital world, a great application of time series data can be on the analysis of visitors to a specific website / blog and predict how many users will that blog or page attract in the future.

In this article, we are going to look at a time series dataset pertaining to users visiting this blog. I'll set up a connection with Google Analytics API in R and bring daily users from mid-Jan 2017 through to mid-May 2017. We'll then create a forecast to predict the number of users this blog will likely attract. I've randomly picked the date range for illustration purposes only.

The idea here is for us to learn how to query data from Google Analytics into R and how to create a time series forecast.

Let's start by setting up our working directory and load the necessary libraries:
```{r warning=FALSE, message=FALSE}
# set working directory
setwd("/Users/FelipeRego/Documents/blog posts/GATimeSeriesInR")

# load the required packages.
library(RGoogleAnalytics) # Library to connect to Google Analytics.
library(ggplot2) # For some initial plots.
library(forecast) # for the time series prediction.
```

We should now set up our Google Analytics connection. There is a very well-written guide [here](https://developers.google.com/analytics/solutions/r-google-analytics) that I used which explains how to set up Google Analytics and connect using R so I'll skip most of the details. In essence, it will show you the package needed and how to perform the necessary query in R to pull data from Google Analytics.

But before that, I had to authorise the RGoogleAnalytics package to fetch data from my user's Google Analytics Account using OAuth2.0. For details, see Auth topic in the package manual [here](https://cran.r-project.org/web/packages/RGoogleAnalytics/RGoogleAnalytics.pdf).

With that sorted, I created a file with my token and client ID details and saved in the directory so I could load it here without displaying my credentials to the public:

```{r warning=FALSE, message=FALSE}
# Load the token object
load("GATimeSeriesInR_oauth_token")
```

With the connection via OAuth sorted, I was able to start querying daily time series data from my blog's users from the Google Analytics API. Below is the initial query that sets the parameters I wanted. In this example, I'm simply pulling users of my blog by day from mid-Jan 2017 to mid-May 2017.

More details can be found of the Google Analytics Developers pages on how to structure different queries to suit other needs [here](https://developers.google.com/analytics/devguides/reporting/core/dimsmets#cats=user).


```{r warning=FALSE, message=FALSE}
# Create a list of the parameters to be used in the Google Analytics query
list_param = Init(start.date = "2017-01-16",
             end.date = "2017-05-14",
             dimensions = "ga:date",
             metrics = "ga:users",
             sort = "ga:date",
             max.results = 10000,
             table.id = "ga:99395442"
             )
```

Once the list of parameters of my query are set, I can then start querying the Google Analytics API using the QueryBuilder and the GetReportData functions:

```{r warning=FALSE, message=FALSE}
# store the results of the Google Analytics query
res = QueryBuilder(list_param)

# get the data from Google Analytics passing query results and oauth
df = GetReportData(res, oauth_token, split_daywise = T)

# re-orders the results 
df = df[order(df$date),]

# inspect the first 30 days of users
head(df,30)
```

As you can see from the 30 records above, the only features or variables used are dates and number of users. It's a very simple dataset but one that serves well to illustrate the time series forecast example.

The GetReportDate function will set the query in motion. In this example, our time series is 120 days long so the query will take a minute or so to run. For longer, more complex queries, expect longer times and beware of API quotas too! The *GetReportDate()* function also allows us to pass a *split_daywise* parameter which paginates the results and break it down on a daily basis.

One of the most important steps in time series analysis is to plot your data and inspect it for any patterns and movements in the series. Let's plot the results our daily users in a time series graph to inspect any trends or seasonality:

```{r warning=FALSE, message=FALSE}
# Deal with dates and plot daily users
df$date <- as.Date(df$date, '%Y%m%d')
df$wkd = as.factor(weekdays(df$date))
ggplot( data = df, aes( date, users )) + 
  geom_line() + 
  ggtitle("Daily Users of this Blog") +
  geom_point(colour = "firebrick", alpha = 0.3) + 
  geom_text(aes(label=  ifelse(  substr(wkd,1,1)=="S", substr(wkd,1,1),""  )  ),hjust=0,vjust=0)
#  geom_text(data=subset(df, wkd==c("S")), aes(label= ifelse(  substr(wkd,1,1) == "S",''  )),hjust=0,vjust=0)
```

Looking at the plot above with data of daily users of this blog, it seems that users have increased over time. The series starts with less than 100 users and goes up to more than 400 users in one day at a given point. There appears to exist a general upward **trend** in users. We can also roughly identify peaks and troughs, or ups and downs in the series. This repeating pattern can be linked to  **seasonal variation**. In other words, there appears to exist some level of seasonality in the number of users visiting this blog on a daily basis. We can run a simple box-plot by day of week to try to better visualise this pattern:

```{r warning=FALSE, message=FALSE}
# create day of week as factor and plot it
df$wkd = factor(df$wkd, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))
ggplot(df, aes(x=wkd, y=users)) + 
  geom_boxplot() +
  geom_jitter(shape=16, position=position_jitter(0.2)) +
  labs(title="User of this blog by Day of Week",x="Day of Week", y = "Users")
```

As you can see in the plot above, Tuesdays through to Thursdays are the biggest days in terms of visitors. It attracts a lot of users, with some mid-week days having achieved more than 400 users at some point. In fact, it appears Thursdays contain a large number of outliers compared to other days of the week. Conversely, Saturdays, Sundays and Mondays attract the lowest number of users compared to other days of the week.

So, when we revisit the time series graph above we can now say that the users tend to visit this blog more during the week (peaks on Thursdays) and less during the weekend (troughs on Sunday). This is the seasonal variation we observed earlier.

In time series analysis, we tend to separate (or **decompose**) the time series data observed into essentially three components: the **trend**, the **seasonal** and the **irregular**. 

We do that so we can observe its characteristics and separate the *signal* from the *noise*. We decompose time series to identify patterns, make estimates, model the data and improve our ability to understand what is going on and predict future behavior. Decomposing time series allows us to fit a model in the data that best describes its behavior. 

The trend component is the long-term direction a time series takes and reflects the underlying level or pattern observed. In our case, the trend is upward which reflects an increasing number of users are visiting the blog.

The seasonal component consists of general effects observed in the data that are consistent in time, magnitude and direction. In our case here, we see frequent positive spikes in users on Wednesdays through to Thursdays for example. Seasonality could be driven by many factors. In retail, seasonality happens in specific dates such as Christmas and Easter. In our blog example, it appears users are visiting more during study / work week and less during the weekend.

The irregular, or also known as the **residual**, is what remains after we remove both the trend and the seasonal components. It reflects unpredictable movements in the series.

Before we decompose our time series of daily users of this blog, we need to transform our data frame of users we queried directly from Google Analytics into a time series object *ts()* in R:

```{r warning=FALSE, message=FALSE}
# transform data frame into a time series object
df_ts = ts(df$users, frequency = 7)

# decompose time series and plot results
decomp = decompose(df_ts, type = "additive")
print(decomp)
plot(decomp)
```

Generally, time series decomposition take the form of either **additive** or **multiplicative**. There are also other forms of decomposition, but we'll not touch on those in this example.

In simplistic terms, additive decompositions are used in time series in which the underlying level of the series fluctuates but the magnitude of the seasonality remains relatively stable. The amplitude of the seasonal and irregular components does not change considerably as the trend level change over time.

On the other hand, multiplicative decompositions are used when the amplitude of the seasonal and irregular increase as the trend increases. 

We can observe in the initial time series plot above that the magnitude of the seasonality remains largely stable across the time series which indicates that an additive decomposition would make more sense. In fact, if you inspect our R script above, you can note that we are using an additive decomposition with the *decompose()* function.

One of our objectives in this exercise is to also try and fit a model that allows us to extrapolate the data and make predictions, forecast of future users of this blog. Obviously, a key assumption in forecasting time series is that the present trend will continue. That is, in the absence of any surprising change or shock, the overall trend should remain similar in the future (at least in the short-term). We'll also disregard any underlying causes for the patterns observed (i.e. any posts from this blog being featured on a Facebook page which had a lot of visibility and may have driven a lot of users to the blog page, for example).

When we perform forecasting in time series, our intention is to predict some future value given a past history of observations up to a certain point in time. Many considerations need to be taken around the form of exponential smoothing required to fit a time series model. We'll touch on only one method here for simplicity sake.

So, considering that our time series has seasonality present and uses an additive decomposition, an appropriate smoothing method is the **Holt-Winters** which uses exponentially weighted moving averages to update estimates.

Let's fit a predictive model in our time series data:

```{r warning=FALSE, message=FALSE}
# apply HoltWinters model in the time series and inspect fit
pred = HoltWinters(df_ts)
print(pred)
```

As we can see from the model fit above, Holt-Winters' smoothing is done using three parameters: **alpha**, **beta** and **gamma**. Alpha estimates the trend (or level) component, the beta estimates the slope of the trend component and gamma estimates the seasonal component. These estimates are based on the latest time point in the series and these are the values that will be used for prediction. The values of alpha, beta and gamma range from 0 to 1, where values close to 0 indicates that recent observations have little weight in the estimates.

From the result above, we can see that the smoothing estimate for alpha is `r pred$alpha`, beta is `r pred$beta` and gamma is `r pred$gamma`. The value of alpha is around 0.5 which indicates that both short-term, recent observations and historical, more distant observations play a part in the trend estimates of the time series. The value of beta is close to zero which indicates that slope (change in level from one time period to the next) of the trend component remain relatively similar throughout the series. The value of gamma is relatively similar to alpha which indicates that the seasonal estimates are based on both recent and distant observations.

The plot of the model fit (red) and the actual observed values (black) below helps to illustrate the results:

```{r warning=FALSE, message=FALSE}
# plot model fit
plot(pred)
```

We can now extrapolate the model to predict future users of this blog:

```{r warning=FALSE, message=FALSE}
# forecast of future users
fcst = forecast.HoltWinters(pred, h=28)
plot.forecast(fcst)
```

Note from the plot above that the thick, bright blue line represents the forecast of users to this blog over the next month or so. The dark blue shaded area represents the 80% prediction interval and the light blue shaded area represents the 95% prediction interval. As we can see, the model generally illustrates the patterns observed and do a relatively good job in estimating future number of users to this blog.

It's recommended we check our forecast model accuracy. First let's look at the **Sum of Squared Errors (SSE)** which is a measure of how far off our model is from the actual observed data. The squaring is performed to avoid negative values and also to give more weight to larger differences. Values closer to 0 are always better. In our example, we can see that the SSE of our model is `r pred$SSE`. 

It is also important to check for **Autocorrelation**. In general, autocorrelation is used to assess whether a correlation exists between the time series and a time lag of that same time series. It's like taking a copy of that time series multiple times and pasting it every step time ahead and assessing whether a pattern is found. Autocorrelation in the context of time series errors is used to try and find patterns that exist in the residuals. This is what the Correlogram below is trying to do:

```{r warning=FALSE, message=FALSE}
# correlogram and histogram of the model residuals
acf(fcst$residuals[8:length(fcst$residual)], lag=60, main="Correlogram of Model Residuals")
hist(fcst$residuals[8:length(fcst$residual)], breaks=10, main="Histogram of Model Residuals")
```

Without spending too much time on the Correlogram itself, we can note that while the model works relatively well, it doesn't do a remarkable job in the early stages of the series as can be seeing in the *acf() function* autocorrelation results (y axis) at lags 2 and 3. 

Additionally, we can see from the histogram of the residuals that the forecast errors are somewhat normally distributed with a presence of some outliers which indicates a relatively good model fit.

Obviously, more tests can and should be performed to test and confirm whether the model is appropriate or can be improved upon, but we'll leave it at that for now.

****

In this blog post we briefly described a simplistic approach to bringing Google Analytics data - in this case time series data - into R and created a simple time series forecasting exercise using user of this blog.
